# Copyright 2022 OmniSafe Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
defaults:
  device: "cpu"
  max_real_time_steps: 1000000
  max_ep_len: 750
  seed: 0
  action_repeat: 1
  obs_clip: 1000
  cost_gamma: 1.0
  # Log
  log_freq: 20000
  data_dir: "./runs"

  # Actor-Critic
  update_policy_freq: 10000
  target_kl: 0.012
  pi_iters: 80
  critic_iters: 80
  pi_lr: 0.0003
  vf_lr: 0.001
  ac_hidden_sizes: [64, 64]

  ## Configuration For Mode
  model_cfgs:
    shared_weights: False
    weight_initialization_mode: "kaiming_uniform"
    ac_kwargs:
      pi:
        hidden_sizes: [64, 64]
        activation: tanh
      val:
        hidden_sizes: [64, 64]
        activation: tanh



  # Virtual roll out
  horizon: 80
  imaging_steps_per_policy_update: 30000
  mixed_real_time_steps: 1500

  # Validation Configuration
  validation_num: 6
  validation_threshold_num: 4
  validation_horizon: 75

  # Optional Configuration
  kl_early_stopping: True
  reward_penalty: False
  scale_rewards: False
  standardized_obs: False


  # Configuration For dynamics model
  update_dynamics_freq: 10000
  dynamics_cfgs:
    network_size: 8
    elite_size: 6
    hidden_size: 200
    use_decay: True

  # Configuration For Off-policy Buffer
  replay_size: 1000000
  batch_size: 0

  # Configuration For On-policy Buffer
  buffer_cfgs:
    gamma: 0.99
    lam: 0.97
    lam_c: 0.97
    adv_estimation_method: "gae-rtg"
    standardized_reward: True
    standardized_cost: True
    reward_penalty: False

  # Configuration For lagrange
  lagrange_cfgs:
    cost_limit: 18.0
    lagrangian_multiplier_init: 0.5
    lambda_lr: 0.05
    lambda_optimizer: "Adam"
    beta: 0.02
